---
title: "MA678 homework 08"
author: "Jiahao Xu"
date: "November 10, 2017"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,out.width="0.9\\linewidth",dev="pdf",fig.align  = 'center')
library(ggplot2)
library(knitr)
library(gridExtra)
library(arm)
library(data.table)
library(foreign)
library(car)
library(stringr)
library(rstan)
library(zoo)

coefplot_my <- function(model){
  toc <- summary(model)$coef
  tab <- data.table(toc)
  tab$coefnames <- rownames(toc)
  tab<-subset(tab,coefnames!="(Intercept)")
  ggplot(tab) + geom_point() + 
    geom_pointrange(aes(ymax = Estimate + 2*`Std. Error` , ymin=Estimate - 2*`Std. Error`),lwd=0.2)+
    aes( y=Estimate, x=coefnames)+geom_pointrange(aes(ymax = Estimate + `Std. Error` , ymin=Estimate - `Std. Error`))+
    geom_hline(yintercept=0,lty	=2)+xlab("coefficients")+ylab("estimate +/- 2 Std.Error")+
    scale_x_discrete(limits=tab$coefnames)+ 
    coord_flip()
}
```


# presidential preference and income for the 1992 election

The folder `nes` contains the survey data of presidential preference and income for the 1992 election analyzed in Section 5.1, along with other variables including sex, ethnicity, education, party identification, political ideology, and state.

1. Fit a logistic regression predicting support for Bush given all these inputs except state. Consider how to include these as regression predictors and also consider possible interactions.

```{r,echo=FALSE}
library(foreign)
brdata <- read.dta("http://www.stat.columbia.edu/~gelman/arm/examples/nes/nes5200_processed_voters_realideo.dta",convert.factors=F)
brdata <- brdata[is.na(brdata$black)==FALSE&is.na(brdata$female)==FALSE&is.na(brdata$educ1)==FALSE
                 &is.na(brdata$age)==FALSE&is.na(brdata$income)==FALSE&is.na(brdata$state)==FALSE,]
kept.cases <- 1952:2000
matched.cases <- match(brdata$year, kept.cases)
keep       <- !is.na(matched.cases)
data       <- brdata[keep,]
plotyear   <- unique(sort(data$year))
year.new   <- match(data$year,unique(data$year))
n.year     <- length(unique(data$year))
income.new <- data$income - 3
age.new    <- (data$age-mean(data$age))/10
y          <- data$rep_pres_intent
data       <- cbind(data, year.new, income.new, age.new, y)
nes.year   <- data[,"year"]
age.discrete <- as.numeric (cut (data[,"age"], c(0,29.5, 44.5, 64.5, 200)))
race.adj     <- ifelse (data[,"race"]>=3, 1.5, data[,"race"])
data        <- cbind (data, age.discrete, race.adj)

data$female <- data[,"gender"] - 1
data$black <- ifelse (data[,"race"]==2, 1, 0)
data$rvote <- ifelse (data[,"presvote"]==1, 0, ifelse(data[,"presvote"]==2, 1, NA))


#sex, ethnicity, education, party identification, political ideology, and state
mod1<-glm(rvote~gender+race+educ1+partyid7+ideo7+ideo7*partyid7, data=data,family="binomial")

```

2.  Now formulate a model predicting support for Bush given the same inputs but allowing the intercept to vary over state. Fit using `lmer()` and discuss your results.

```{r,echo=FALSE}
mod11<-lmer(rvote~gender+race+educ1+partyid7+ideo7+ideo7*partyid7+(1|state), data=data)
summary(mod11)


```

3. Create graphs of the probability of choosing Bush given the linear predictor associated with your model separately for each of eight states as in Figure 14.2.

```{r,echo=FALSE}


```



## Three-level logistic regression: 

the folder `rodents` contains data on rodents in a sample of New York City apartments.

1. Build a varying intercept logistic regression model (varying over buildings) to predict the presence of rodents (the variable rodent2 in the dataset) given indicators for the ethnic groups (race) as well as other potentially relevant predictors describing the apartment and building. Fit this model using lmer() and interpret the coefficients at both levels.

```{r,echo=FALSE}
apt.subset.data <- read.table ("http://www.stat.columbia.edu/~gelman/arm/examples/rodents/rodents.dat", header=TRUE)
apt_dt <- data.table(apt.subset.data)

invisible(apt_dt[,asian := race==5 | race==6 | race==7])
invisible(apt_dt[,black := race==2])
invisible(apt_dt[,hisp  := race==3 | race==4])

mod2<-lmer(rodent2~stories+housewgt+subsidy+housing+old+help+(1|race)+(1|borough) ,data=apt_dt, family="binomial")
display(mod2)

# According to the summary,  races will differ in 0.48/4=12% in having rodents and boroghs will differ in 0.36/4=9% in having rodents
``` 

2. Now extend the model in (1) to allow variation across buildings within community district and then across community districts. Also include predictors describing the community districts. Fit this model using lmer() and interpret the coefficients at all levels.

```{r,echo=FALSE}
mod22<-lmer(rodent2~stories+housewgt+subsidy+housing+old+help+race+ (1 | borough) + (1 | cd) ,data=apt_dt, family="binomial")
display(mod22)
# According to the summary,  cd will differ in 0.67/4=17% in having rodents and boroghs will differ in 0.42/4=10.5% in having rodents
```

3. Compare the fit of the models in (1) and (2).

```{r,echo=FALSE}
anova(mod2,mod22)
# According to the anova summary, mod22 is not obviously better than mod2
```

## Item-response model: 

the folder `exam` contains data on students' success or failure (item correct or incorrect) on a number of test items. Write the notation for an item-response model for the ability of each student and level of difficulty of each item.

```{r,echo=FALSE}
# Read in the data from an excel-format ".csv" file
exam.data.raw <- read.table("http://www.stat.columbia.edu/~gelman/arm/examples/exam/mtermgrades.txt", header=FALSE)

```

##  Multilevel logistic regression 

The folder `speed.dating` contains data from an experiment on a few hundred students that randomly assigned each participant to 10 short dates with participants of the opposite sex (Fisman et al., 2006). For each date, each person recorded several subjective numerical ratings of the other person (attractiveness, compatibility, and some other characteristics) and also wrote down whether he or she would like to meet the other person again. Label $y_{ij} = 1$ if person $i$ is interested in seeing person $j$ again $0$ otherwise.
And $r_{ij1},\dots, r_{ij6}$ as person $i$'s numerical ratings of person $j$ on the dimensions of attractiveness, compatibility, and so forth.
Please look at 
http://www.stat.columbia.edu/~gelman/arm/examples/speed.dating/Speed%20Dating%20Data%20Key.doc
for details.

```{r}
dating<-fread("http://www.stat.columbia.edu/~gelman/arm/examples/speed.dating/Speed%20Dating%20Data.csv")

```

1. Fit a classical logistic regression predicting $Pr(y_{ij} = 1)$ given person $i$'s 6 ratings of person $j$. Discuss the importance of attractiveness, compatibility, and so forth in this predictive model.

```{r}
mod3<- glm(match~attr_o +sinc_o +intel_o +fun_o +amb_o +shar_o,data=dating,family="binomial")
summary(mod3)
coefplot(mod3)
# From the summary of the model, we get to know that the probability is  P(match=1)=invlogit(-5.62+0.22*attr-0.02*sinc+0.07*inter+0.25*fun-0.12*amb+0.21*shar)
# Interpretation: If 6 indicators are all zero, then the probability is invlogit(-5.62)
# Each increase in attr will lead to 5.5%(0.22/4) higher match
# Each increase in sinc will lead to 0.5%(0.02/4) lower match
# Each increase in intel will lead to 1.4%(0.07/4) higher match
# Each increase in fun will lead to 6.25%(0.25/4) higher match
# Each increase in amb will lead to 3%(0.12/4) lower match
# Each increase in shar will lead to 5.25%(0.21/4) higher match
```

2. Expand this model to allow varying intercepts for the persons making the evaluation; that is, some people are more likely than others to want to meet someone again. Discuss the fitted model.

```{r}
mod33<- lmer(match~gender+partner+scale(attr_o) +scale(sinc_o) +scale(intel_o) +scale(fun_o) +scale(amb_o) +scale(shar_o)+(1|id),data=dating,family=binomial(link="logit"))
summary(mod33)
# P(match=1)=invlogit(-2.05+0.14*gender+0.01*partner+0.45*scale(attr)-0.03*scale(sinc)+0.10*scale(inter)+0.50*scale(fun)-0.23*scale(amb)+0.45*scale(shar))
# Interpretation: If 8 indicators are all zero, then the probability is invlogit(-2.05)
# Each increase in gender will lead to 3.5%(0.14/4) higher match
# Each gender in gender will lead to 0.25%(0.01/4) higher match
# Each increase in attr will lead to 11.25%(0.45/4) higher match
# Each increase in sinc will lead to 0.75%(0.03/4) lower match
# Each increase in intel will lead to 2.5%(0.10/4) higher match
# Each increase in fun will lead to 12.5%(0.50/4) higher match
# Each increase in amb will lead to 5.7%(0.23/4) lower match
# Each increase in shar will lead to 10.25%(0.45/4) higher match
```

3. Expand further to allow varying intercepts for the persons being rated. Discuss the fitted model.

```{r}
mod333 <- glmer(match~gender+partner+scale(attr_o) +scale(sinc_o) +scale(intel_o) +scale(fun_o) +scale(amb_o) +scale(shar_o)+(1|id)+(1|idg),data=dating,family=binomial(link="logit"))
summary(mod333)
# P(match=1)=invlogit(-2.07+0.14*gender+0.01*partner+0.45*scale(attr)-0.03*scale(sinc)+0.10*scale(inter)+0.50*scale(fun)-0.23*scale(amb)+0.46*scale(shar))
# Interpretation: If 8 indicators are all zero, then the probability is invlogit(-2.07)
# Each increase in gender will lead to 3.5%(0.14/4) higher match
# Each gender in gender will lead to 0.25%(0.01/4) higher match
# Each increase in attr will lead to 11.25%(0.45/4) higher match
# Each increase in sinc will lead to 0.75%(0.03/4) lower match
# Each increase in intel will lead to 2.5%(0.10/4) higher match
# Each increase in fun will lead to 12.5%(0.50/4) higher match
# Each increase in amb will lead to 5.7%(0.23/4) lower match
# Each increase in shar will lead to 10.5%(0.46/4) higher match
# The interpretations of Coefficient are almost the same as mod2.
```

4. You will now fit some models that allow the coefficients for attractiveness, compatibility, and the other attributes to vary by person. Fit a no-pooling model: for each person i, fit a logistic regression to the data $y_{ij}$ for the 10 persons j whom he or she rated, using as predictors the 6 ratings $r_{ij1},\dots,r_{ij6}$ . (Hint: with 10 data points and 6 predictors, this model is difficult to fit. You will need to simplify it in some way to get reasonable fits.)
```{r}
mod3333 <- glm(match~attr_o + sinc_o + intel_o + fun_o + amb_o + shar_o + factor(iid)-1,data=dating)
summary(mod3333)
# According to the summary(mod3333), we know that mod4 is varied by person
```


5. Fit a multilevel model, allowing the intercept and the coefficients for the 6 ratings to vary by the rater i.

```{r}
mod33333 <- glmer(match~(1+attr_o+sinc_o+intel_o+fun_o+amb_o+shar_o|iid) + attr_o + sinc_o + intel_o + fun_o + amb_o + shar_o,data=dating)
summary(mod33333)
```

6. Compare the inferences from the multilevel model in (5) to the no-pooling model in (4) and the complete-pooling model from part (1) of the previous exercise.

```{r}
anova(mod33333,mod3333,mod3)
```



## The well-switching data described in Section 5.4 are in the folder arsenic.

1. Formulate a multilevel logistic regression model predicting the probability of switching using log distance (to nearest safe well) and arsenic level and allowing intercepts to vary across villages. Fit this model using `lmer()` and discuss the results.

```{r,echo=FALSE}

village <- read.delim("http://www.stat.columbia.edu/~gelman/arm/examples/arsenic/Village.txt",header=TRUE,dec = ",")
as.double(gsub(",","",village$Best.Longitude))
ggplot(village)+geom_jitter()+aes(x=long,y=lat)
wells <- read.table("http://www.stat.columbia.edu/~gelman/arm/examples/arsenic/wells.dat", header=TRUE)
wells <- read.dta("http://www.stat.columbia.edu/~gelman/arm/examples/arsenic/all.dta",convert.factors=F)
wells_f <- read.csv("http://www.stat.columbia.edu/~gelman/arm/examples/arsenic/All.csv", header=TRUE)
wells_f <- read.csv("http://www.stat.columbia.edu/~gelman/arm/examples/arsenic/fulldata1.csv", header=TRUE)

mod4<-lmer(switch~log(distcw)+ arsenic +(1|id),data=wells,family="binomial")
display(mod4)
```

2. Extend the model in (1) to allow the coefficient on arsenic to vary across village, as well. Fit this model using `lmer()` and discuss the results.

```{r,echo=FALSE}

mod44<-lmer(switch~log(distcw)+ arsenic +(1|id)+(0+arsenic|id),data=wells,family="binomial")
display(mod44)
```

3. Create graphs of the probability of switching wells as a function of arsenic level for eight of the villages.

```{r,echo=FALSE}


```

4. Compare the fit of the models in (1) and (2).

```{r,echo=FALSE}
anova(mod4,mod44)
# According to the anova summary, model in (2) is much better that that in (1)
```
